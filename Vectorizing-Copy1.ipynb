{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing\n",
    "\n",
    "Text Analysis is a major application field for machine learning algorithms. However the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect **numerical feature vectors with a fixed size** rather than the **raw text documents with variable length**.\n",
    "\n",
    "In order to address this, scikit-learn provides utilities for the most common ways to extract numerical features from text content, namely:\n",
    "\n",
    "   * **tokenizing** strings and giving an integer id for each possible token, for instance by using white-spaces and punctuation as token separators.\n",
    "   * **counting** the occurrences of tokens in each document.\n",
    "   * **normalizing and weighting** with diminishing importance tokens that occur in the majority of samples / documents.\n",
    "\n",
    "A set of documents can thus be represented by a matrix with one row per document and one column per token (e.g. word) occurring in the corpus.\n",
    "\n",
    "We call **vectorization** the general process of turning a collection of text documents into numerical feature vectors. Documents are described by **word occurrences** while completely **ignoring the relative position information** of the words in the document.\n",
    "\n",
    "We want an algebraic model representing textual information as a vector, the components of this vector could represent the absence or presence (Bag of Words) of it in a document or even the importance of a term (tf–idf) in the document. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## CountVectorizer\n",
    "\n",
    "The first step in modeling the document into a vector is to create a dictionary of terms present in documents. To do that, you can simple select all terms from the document and convert it to a dimension in the vector space, but we know that there are some kind of words (stop words) that are present in almost all documents, and what we’re doing is extracting important features from documents, features do identify them among other similar documents, so using terms like “the, at, on”, etc.. isn’t going to help us, so in the information extraction, we’ll just ignore them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take the documents below to define our (stupid) document space:\n",
    "\n",
    ">**Train Document Set:**\n",
    "\n",
    ">d1: The sky is blue.<br>\n",
    ">d2: The sun is bright.\n",
    "\n",
    ">**Test Document Set:**\n",
    "\n",
    ">d3: The sun in the sky is bright.<br>\n",
    ">d4: We can see the shining sun, the bright sun.\n",
    "\n",
    "```python\n",
    "\n",
    "    ['sky', 'blue', 'sun', 'bright']\n",
    "```\n",
    "### Indexed Vocalbulary or Dictionary vectorizer (not much important but I like talking about it)\n",
    "\n",
    "we’re going to use the **term-frequency** to represent each term in our vector space; the term-frequency is nothing more than a measure of how many times the terms present in our vocabulary, are present in the documents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = (\"The sky is blue.\", \"The sun is bright.\")\n",
    "test_set = (\"The sun in the sky is bright.\",\n",
    "    \"We can see the shining sun, the bright sun.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **CountVectorizer** already uses as default “analyzer” called **WordNGramAnalyzer**, which is responsible to convert the text to lowercase, accents removal, token extraction, filter stop words, etc… you can see more information by printing the class information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s create now the vocabulary index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 5, 'sky': 3, 'is': 2, 'blue': 0, 'sun': 4, 'bright': 1}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(train_set)\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s use the same vectorizer now to create the vectors of our test_set documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The sun in the sky is bright.', 'We can see the shining sun, the bright sun.')\n"
     ]
    }
   ],
   "source": [
    "print(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 1, 2],\n",
       "       [0, 1, 0, 0, 2, 2]], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vec = vectorizer.transform(test_set)\n",
    "#print(test_vec)\n",
    "test_vec.toarray()\n",
    "# type(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform(['The ball is red']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['bright', 'is', 'sky', 'sun', 'the'], dtype='<U6'),\n",
       " array(['bright', 'sun', 'the'], dtype='<U6')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.inverse_transform(test_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the main problem with the term-frequency approach is that it **scales up frequent terms** and **scales down rare terms** which are empirically more informative than the high frequency terms. The basic intuition is that a term that occurs frequently in many documents is not a good discriminator, and really makes sense (at least in many experimental tests); the important question here is: why would you, in a classification problem for instance, emphasize a term which is almost present in the entire corpus of your documents ?\n",
    "\n",
    "Two problems with this approach:\n",
    "   * document size is not taken into consideration. (normalization)\n",
    "   * document frequency of words are also ignored. (weighting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf–idf term weighting\n",
    "\n",
    "The tf-idf weight comes to solve this problem. What tf-idf gives is how important is a word to a document in a collection, and that’s why tf-idf incorporates local and global parameters, because it takes in consideration not only the isolated term but also the term within the document collection. What tf-idf then does to solve that problem, is to scale down the frequent terms while scaling up the rare terms; a term that occurs 10 times more than another isn’t 10 times more important than it, that’s why tf-idf uses the logarithmic scale to do that.\n",
    "\n",
    "The use of this simple term frequency could lead us to problems like keyword spamming, which is when we have a repeated term in a document with the purpose of improving its ranking on an IR (Information Retrieval) system or even create a bias towards long documents, making them look more important than they are just because of the high frequency of the term in the document.\n",
    "\n",
    "To overcome this problem, the term frequency of a document on a vector space is usually normalized.(**vector Normalization**)\n",
    "\n",
    "Let’s see now, how idf (inverse document frequency) is then defined:\n",
    "\n",
    "![](img/tf-idf.png)\n",
    "\n",
    "where,\n",
    "   * |D| Number of documents \n",
    "   * |{d : t in d}| is the number of documents where the term t appears, when the term-frequency function satisfies tf(t,d) != 0, we’re only adding 1 into the formula to avoid zero-division.\n",
    "\n",
    "The formula for the tf-idf is then:\n",
    "\n",
    "![](img/tf-idf2.png)\n",
    "\n",
    "and this formula has an important consequence: a high weight of the tf-idf calculation is reached when you have a high term frequency (tf) in the given document (local parameter) and a low document frequency of the term in the whole collection (global parameter).\n",
    "\n",
    "### Lets try implementing it in python\n",
    "\n",
    "The first step is to create our training and testing document set and computing the term frequency matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = (\"The sky is blue.\", \"The sun is bright.\")\n",
    "test_set = (\"The sun in the sky is bright.\",\n",
    "\"We can see the shining sun, the bright sun.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 5, 'sky': 3, 'is': 2, 'blue': 0, 'sun': 4, 'bright': 1}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(train_set)\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.57615236, 0.        , 0.40993715, 0.57615236, 0.        ,\n",
       "        0.40993715],\n",
       "       [0.        , 0.57615236, 0.40993715, 0.        , 0.57615236,\n",
       "        0.40993715]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform(train_set).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.42519636, 0.30253071, 0.42519636, 0.42519636,\n",
       "        0.60506143],\n",
       "       [0.        , 0.37729199, 0.        , 0.        , 0.75458397,\n",
       "        0.53689271]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform(test_set).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing a large text corpus with the hashing trick (self reading) !!\n",
    "\n",
    "# References\n",
    "\n",
    "   * [Feature Extraction sklearn docs](http://scikit-learn.org/dev/modules/feature_extraction.html)\n",
    "   * Reference Documentations for [CountVectorizer](http://scikit-learn.org/dev/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer), [tfidfVectorizer](http://scikit-learn.org/dev/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer) and [DictVectorizer](http://scikit-learn.org/dev/modules/generated/sklearn.feature_extraction.DictVectorizer.html#sklearn.feature_extraction.DictVectorizer)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
