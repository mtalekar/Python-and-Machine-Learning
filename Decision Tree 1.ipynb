{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "   \n",
    "### Introduction\n",
    "A decision tree uses a tree structure to represent a number of possible decision paths and an outcome for each path. At each node, a question is asked and based on the answer of the question the path is selected.<br>\n",
    "<img src=\"images/image_1.png\" width=\"500\">\n",
    "\n",
    "   * Decision Nodes (Internal nodes)\n",
    "   * Class Labels (Leaf nodes)\n",
    "   \n",
    "### Problem Statement\n",
    "Lets Start by framing Our Problem statement. You are working in HR department of a XYZ company. Your boss gives you a file, asks you to go through all the records and come up with list of some potential candidates to call for hiring.\n",
    "\n",
    "Since you are completely new to the company; you go to your senior from college and ask him for help. He gives you the above decision tree. This is what you identified on your own.\n",
    "\n",
    "   * Features set = [ Level, Language, Twitter, PhD! ]\n",
    "       - level = { junior, Mid, Senior }\n",
    "       - Language = { Java, Python, R }\n",
    "       - Twitter = { Yes, No }\n",
    "       - PhD = { Yes,No }\n",
    "   * Class/Label = [ call for hiring, Don't call for hiring ]\n",
    "   \n",
    "Q. [ Mid, Python, Yes, No ]<br>\n",
    "Q. [ Senior, Java, Yes, No ]<br>\n",
    "Q. [ Junior, R, Yes, Yes ]<br>\n",
    "\n",
    "<img src=\"images/image_1.png\" width=\"480\">\n",
    "<br>\n",
    "Clearly, we now know how to use a given decision tree to classify any new data point. But...<br>\n",
    "Here, the tree was already given to us. What if the tree was not available with us? How can be build this tree ?\n",
    "\n",
    "## Demo Time !!!\n",
    "\n",
    "## Advantages and DisAdvantages of DTs\n",
    "For more details refer [this](http://scikit-learn.org/stable/modules/tree.html).\n",
    "\n",
    "### Advantages\n",
    "   * Simple to understand and to interpret. Trees can be visualised.\n",
    "   * No feature normalization or scaling needed.\n",
    "   * Works well with datasets using mixed feature types (like categorical, binary and continuous).\n",
    "   * The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.\n",
    "   * Uses a white box model. If a given situation is observable in a model, the explanation for the condition is easily explained by boolean logic.\n",
    "   \n",
    "### DisAdvantages\n",
    "   * There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems.\n",
    "   * Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble.\n",
    "   \n",
    "## Practical Tips\n",
    "Refer [this](http://scikit-learn.org/stable/modules/tree.html#tips-on-practical-use)\n",
    "   \n",
    "## Linear model v/s Decision Tree\n",
    "![](images/image_7.png)\n",
    "\n",
    "   * A two-dimensional classiﬁcation example in which the true decision boundary is linear, and is indicated by the shaded regions. A classical approach that assumes a linear boundary (left) will outperform a decision tree that performs splits parallel to the axes (right).<br> \n",
    "   * If the true decision boundary is non-linear. Here a linear model is unable to capture the true decision boundary (left), whereas a decision tree is successful (right).\n",
    "\n",
    "   \n",
    "## ID3 algorithm to build a decision tree \n",
    "\n",
    "There are many possible decision trees which can classify you data correctly but the best decision tree is the one which has the **least height** and **least number of decision nodes**.\n",
    "\n",
    "General Idea is to select sequence of features that give you more certain results in less steps. For example\n",
    "\n",
    "![](images/image_6.png)\n",
    "\n",
    "### Entropy (H)\n",
    "**Entropy H(S)** is a measure of the amount of uncertainty in the (data).<br>\n",
    "\n",
    "<img src=\"images/image_2.png\" width=\"250\">\n",
    "\n",
    "Where,<br>\n",
    "**S** – The current (data) set for which entropy is being calculated (changes every iteration of the ID3 algorithm)<br>\n",
    "**X** – Set of classes in **S** <br>\n",
    "**p(x)** – The proportion of the number of elements in class **x** to the number of elements in set **S**\n",
    "\n",
    "![](images/image_3.png)\n",
    "\n",
    "In ID3, entropy is calculated for each remaining attribute. The attribute with the smallest entropy is used to split the set on that particular iteration.\n",
    "\n",
    "Q. Entropy when,\n",
    "   * level = Mid(4,0)\n",
    "   * Language = Python(5,2)\n",
    "\n",
    "### Information Gain (IG)\n",
    "Information gain IG(A) is the measure of the difference in entropy from before to after the set is split on an attribute. In other words, how much uncertainty in was reduced after splitting S set on attribute A.\n",
    "\n",
    "![](images/image_4.png)\n",
    "\n",
    "Where,<br>\n",
    "**H(S)** – Entropy of set **S**.<br>\n",
    "**T** – The subsets created from splitting set **S** by attribute **A**.<br>\n",
    "**p(t)** – The proportion of the number of elements in t to the number of elements in set **S**.<br>\n",
    "**H(t)** – Entropy of subset **t**.<br> \n",
    "\n",
    "In ID3, information gain can be calculated (instead of entropy) for each remaining attribute. The attribute with the largest information gain is used to split the set S on this iteration. \n",
    "\n",
    "## Solved Example\n",
    "\n",
    "<img src=\"images/image_5.png\" width=\"950\" height=\"550\">\n",
    "\n",
    "## Key parameters\n",
    "\n",
    "- *max_depth*: controls maximum depth (number of split points). Most common way to reduce tree complexity and overfitting.\n",
    "- *min_samples_leaf*: threshold for minimum number of data instances a leaf can have to avoid further splitting.\n",
    "- *max_leaf_nodes*: limits total number of leaves in the tree.\n",
    "\n",
    "## References\n",
    "\n",
    "   * [Sklearn User Guide](http://scikit-learn.org/stable/modules/tree.html)\n",
    "   * [Sklearn Decision Tree Source code](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)\n",
    "   * [Decision Tree Wiki](https://en.wikipedia.org/wiki/Decision_tree)\n",
    "   * [Solved Example (pdf)](https://storage.googleapis.com/supplemental_media/udacityu/313488098/ID3%20Algorithm%20for%20Decision%20Trees.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembles\n",
    "\n",
    "Refer [this](https://scikit-learn.org/stable/modules/ensemble.html#ensemble-methods) for detail.\n",
    "\n",
    "A widely used and effective method in machine learning involves creating learning models known as **ensembles**. An ensemble takes multiple individual learning models and combines them to produce an aggregate model that is more powerful than any of its individual learning models alone. Why are ensembles effective? Well, one reason is that if we have different learning models, although each of them might perform well individually, they'll tend to make different kinds of mistakes on the data set. And typically, this happens because each individual model might overfit to a different part of the data. By combining different individual models into an ensemble, we can average out their individual mistakes to reduce the risk of overfitting while maintaining strong prediction performance.\n",
    "\n",
    "## Random forest\n",
    "\n",
    "![](images/rf_process.png)\n",
    "\n",
    "##### 1. Bootstrap samples\n",
    "##### 2. *max_features* parameter\n",
    "Learning is quite sensitive to *max_features*. Setting *max_features* = 1, leads to forests with more diverse, more complex trees. Setting *max_features* = #features, will lead to similar forests with similar trees.\n",
    "##### 3. Prediction Using Random Forests\n",
    "Make predictions for every tree in the forest. Combine individual predictions [probabilities averages across trees, predict the class with highest probability].\n",
    "\n",
    "### Advantages\n",
    "- Widely used, excellent prediction preformance on many problems\n",
    "- Does not require feature normalization like other linear models\n",
    "- Like Decision tree, handles a mixture of feature types.\n",
    "- Easily parallized across multiple CPUs.\n",
    "\n",
    "### Disadvantages\n",
    "- Resulting models are usually difficult for humans to interpret.\n",
    "- Like Decision tree, random forest are not good choice for very high dimensional tasks (like text data) compared to fast, more accurate linear models. \n",
    "\n",
    "### Key parameters\n",
    "- n_estimators: number of trees to use in ensembel (default: 10). It should be larger for larger datasets to reduce overfitting (but uses more computational).\n",
    "- max_features: has a strong effect on performance. Influences the diversity of trees in the forest.\n",
    "- max_depth: controls the depth of each tree (default: None, splits untill all leaves are pure).\n",
    "- n_jobs: how many cores to use in parallel during training.\n",
    "* Choose a fixed setting for the random_state parameter if you need reproducible results.\n",
    "### References\n",
    "- [Blog](https://towardsdatascience.com/the-random-forest-algorithm-d457d499ffcd)\n",
    "- [Wikipedia](https://en.wikipedia.org/wiki/Random_forest)\n",
    "- [Sklearn User Guide](https://scikit-learn.org/stable/modules/ensemble.html#forests-of-randomized-trees)\n",
    "\n",
    "## Gradient Boosted Decesion Tree\n",
    "\n",
    "![](images/gbt.png)\n",
    "\n",
    "A **weak learner** is defined to be a classifier that is only slightly correlated with the true classification (it can label examples better than random guessing). In contrast, a **strong learner** is a classifier that is arbitrarily well-correlated with the true classification.\n",
    "\n",
    "**Gradient boosting** is a machine learning technique, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion where each tree attempts to correct errors from the previous stage.\n",
    "\n",
    "**Learning Rate** controls how hard each new tree tries to correct remaining mistakes from previous round.\n",
    "- High Learning rate: more complex trees.\n",
    "- Low Learning rate: simpler trees.\n",
    "\n",
    "### Advantages\n",
    "- Often best off-the-shelf accuracy on many classification problems.\n",
    "- Requires only modest memory and is fast.\n",
    "- Does not require normalization of features to perform well.\n",
    "- Handles a mixture of feature types.\n",
    "\n",
    "### Disadvantages\n",
    "- Like Random forests, the models are often difficult for humans to interpret.\n",
    "- requires carefull tuning of the learning rate and other parameters.\n",
    "- Like Decision trees, not recommended for text or other problems with very high dimensional sparse features, for accuracy and computational cost reasons.\n",
    "\n",
    "### Key parameters\n",
    "- *n_estimators*: sets number of small decision trees to use (weak learners) in teh ensemble.\n",
    "- *learning_rate*: controls emphasis on fixing errors from pervious iterations.\n",
    "- The above two are typically tuned together.\n",
    "- *n_estimators*: is adjusted first, to best exploit memory and CPUs during training, then other parameters.\n",
    "- *max_depth*: is typically set to a small value for most applications.\n",
    "\n",
    "### References\n",
    "- [Boosting (machine learning)](https://en.wikipedia.org/wiki/Boosting_(machine_learning)\n",
    "- [Gradient Boosting](https://en.wikipedia.org/wiki/Gradient_boosting)\n",
    "- [Gradient Boosted tree in sklearn](http://scikit-learn.org/stable/modules/ensemble.html#gradient-tree-boosting)\n",
    "\n",
    "# Research paper\n",
    "- [A Few Useful Things to Know about Machine Learning](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
